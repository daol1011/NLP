{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing - Summer Term 2024\n",
    "### Hochschule Karlsruhe\n",
    "### Lecturer: Prof. Dr. Jannik Strötgen\n",
    "### Tutor: Paul Löhr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will learn about:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenization\n",
    "- data cleaning and stop word removal\n",
    "- stemming\n",
    "- zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Tokenization (5 P):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "Describe what tokenization is, how it is performed, and what problems it solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 100 words\n",
    " \n",
    " From the lecture:\n",
    "\n",
    " Token:\n",
    " ● The occurrence of a word in a text\n",
    " Tokenization:\n",
    " ● Segmentation of an input stream into an ordered sequence of tokens\n",
    " Tokenizer: \n",
    "● A system that splits texts into word tokens\n",
    " Example:\n",
    " ● Input text: John likes Mary and Mary likes John.\n",
    " ● Tokens: {“John”, “likes”, “Mary”, “and”, “Mary”, “likes”, “John”, “.”}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "For the later analysis of each text file, we need to identify single tokens. Therefore, you have to use a library to separate single tokens from the text. We will use the methods offered by `nltk` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dawso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dawso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/texts.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "content_debates = data['debates']\n",
    "content_reddit = data['reddit']\n",
    "content_tv = data['tv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens DEBATE:  ['Good', 'evening', 'from', 'Hofstra', 'University', 'in', 'Hempstead', ',', 'New', 'York', '.', 'I', 'am', 'Lester', 'Holt', ',', 'anchor', 'of', '``', 'NBC']\n",
      "Tokens REDDIT:  ['*', '*', 'ALL', 'USERS', ':', 'PLEASE', 'CONSIDER', 'THIS', 'YOUR', 'ONE', 'AND', 'ONLY', 'WARNING', '*', '*', 'This', 'is', 'a', 'reminder', 'that']\n",
      "Tokens TV:  ['``', 'THE', 'TERMS', 'WERE', 'LAID', 'OUT', '.', 'I', 'WROTE', '--', '>', '>', 'YOU', 'CALLED', 'IT', 'THE', 'GOLD', 'STANDARD', '.', \"''\"]\n",
      " Good evening from Hofstra University in Hempstead, New York. I am Lester Holt, anchor of \"NBC Nightly News.” I want to welcome you to the first presidential debate.\n",
      "The participants tonight are Donald Trump an \n",
      "\n",
      "**ALL USERS: PLEASE CONSIDER THIS YOUR ONE AND ONLY WARNING**\n",
      "\n",
      "This is a reminder that this subreddit has strict posting/commenting rules that will be enforced by moderation. If you are new to this su \n",
      "\n",
      "\"THE TERMS WERE LAID OUT. I WROTE -- >> YOU CALLED IT THE GOLD STANDARD.\"\n",
      "  YEAH.\"\n",
      "  SECRETARY CLINTON. >> I HAVE A FEELING BY THE END OF THIS\"\n",
      " \"THAT ARE INEFFECTIVE. STOP AND FRISK WAS FOUND TO BE U \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenize the text content for the three datasets above\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens_debates = word_tokenize(content_debates)\n",
    "tokens_reddit = word_tokenize(content_reddit)\n",
    "tokens_tv = word_tokenize(content_tv)\n",
    "\n",
    "\n",
    "# 2. Print the first 20 tokens for each dataset\n",
    "\n",
    "# direct print\n",
    "# print(\"Tokens DEBATE: \", tokens_debates[:20])\n",
    "# print(\"Tokens REDDIT: \", tokens_reddit[:20])\n",
    "# print(\"Tokens TV: \", tokens_tv[:20])\n",
    "# save as variables\n",
    "tokens_debates_f20 = tokens_debates[:20]\n",
    "tokens_reddit_f20 = tokens_reddit[:20]\n",
    "tokens_tv_f20 = tokens_tv[:20]\n",
    "\n",
    "print(\"Tokens DEBATE: \", tokens_debates_f20)\n",
    "print(\"Tokens REDDIT: \", tokens_reddit_f20)\n",
    "print(\"Tokens TV: \", tokens_tv_f20)\n",
    "\n",
    "\n",
    "# 3. Now display the first paragraphs of the corresponding original text and study them. \n",
    "print(content_debates[:210],\"\\n\")\n",
    "print(content_reddit[0:200],\"\\n\")\n",
    "print(content_tv[0:200],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE \n",
    "# 1. \n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Does this what you expected it to do? How well does the tokenization work? What happens to special characters? Can you think of any problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)\n",
    "- Several stop words are not yet tokenized\n",
    "- Special characters are tokenized too\n",
    "- Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Data Cleaning and Stop Word Removal (10 P):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In two to three sentences, describe what *data cleaning* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)\n",
    "\n",
    "- Detecting and correcting corrupt, inaccurate or irrelevant data records.\n",
    "- After cleaning, should be consistent to the other data sets in the system.\n",
    "- It is the prerequisite for the proper analysis afterwards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "To have more accurate word counts and visualizations, it is often helpful to remove the capitalization of words. This is especially true for languages like German. In the following, for the three texts from above, remove any capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE\n",
    "    \n",
    "# def conv_to_lower(tokens):\n",
    "#     tokens_lower = []\n",
    "#     for token in tokens:\n",
    "#         tokens_lower.append(token.lower())\n",
    "#     return tokens_lower \n",
    "\n",
    "# print(conv_to_lower(tokens_reddit_f20))\n",
    "# print(conv_to_lower(tokens_debates_f20))\n",
    "# print(conv_to_lower(tokens_tv_f20))\n",
    "\n",
    "reddit_lower = content_reddit.lower()\n",
    "debates_lower = content_debates.lower()\n",
    "tv_lower = content_tv.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Apply tokenization to the lowercase version of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE\n",
    "reddit_lower_tok = word_tokenize(reddit_lower)\n",
    "debates_lower_tok = word_tokenize(debates_lower)\n",
    "tv_lower_tok = word_tokenize(tv_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "In two to three sentences, describe what *stop word removal* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)\n",
    "It is a common preprocessing step in NLP. The idea is to remove very common words that occur across all documents and carry little to no information. These tend to be things like articles or pronouns for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Now apply stop word removal to the three datasets.\n",
    "\n",
    "Hint: Assume the texts are all written in _English_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE\n",
    "op_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "content_reddit_rm_sw = [token for token in reddit_lower_tok if not token in op_words]\n",
    "content_debates_rm_sw = [token for token in debates_lower_tok if not token in op_words]\n",
    "content_tv_rm_sw = [token for token in tv_lower_tok if not token in op_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "Now compare the first original sentence for each dataset with the parts remaining after performing the above steps. Write them down and explain what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**ALL USERS: PLEASE CONSIDER THIS YOUR ONE AND ONLY WARNING**\n",
      "\n",
      "This is a reminder that this subreddit has strict posting/commenting rules that will be enforced by moderation. If you are new to this su\n",
      "['*', '*', 'users', ':', 'please', 'consider', 'one', 'warning', '*', '*', 'reminder', 'subreddit', 'strict', 'posting/commenting', 'rules', 'enforced', 'moderation', '.', 'new', 'subreddit', ',', 'please', 'read', 'sidebar', 'commenting', '.', '-', \"n't\", 'post', 'low', 'effort', 'comments', '(', 'ie-', 'jokes', ',', 'memes', ',', 'slogans', ',', 'links', 'without', 'context', ')', '.', '-', 'personally', 'insult', 'redditors', '.', 'post', 'racist', ',', 'sexist', ',', 'homophobic', ',', 'bigoted', ',', 'otherwise', 'discriminatory', 'content', '.', '-', 'meta', 'discussion', '(', 'discussion', 'subreddits', ',', 'users', ',', 'moderators', ',', 'etc', '.', ')', 'allowed', '.', 'rule', 'violations', 'today/tonight', '*', '*', 'moderated', '*', '*', ',', '``', 'know/understand', 'rules', \"''\", 'considered', 'valid', 'excuse', 'violating', '.', 'understand', 'exciting', 'time']\n"
     ]
    }
   ],
   "source": [
    "# CODE SUBMISSION ANSWER HERE\n",
    "\n",
    "print(content_reddit[:200])\n",
    "print(content_reddit_rm_sw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)\n",
    "- All words are lowercase\n",
    "- All stop words have been removed\n",
    "- Some special characters remain like '*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Stemming (10 P):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "In two to three sentences, describe what *stemming* in the context of text data refers to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)\n",
    "- Reduce response time for indexing by truncating words to their root. \n",
    "- Reducing inflected words to their stem.\n",
    "- The last few characters (suffix) of a given words are removed to obtain a shorter form.\n",
    "- Goal is to reduce the number of unique words by grouping word variations based on their root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Think about how you would go about implementing your own stemmer?\n",
    "Come up with at least ten rules and write them down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: For example:\n",
    "\n",
    "```*s -> *   # remove trailing s```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) \n",
    "```*e -> *   # remove trailing e```  \n",
    "```*ed -> *   # remove trailing ed```  \n",
    "```*er -> *   # remove trailing er```  \n",
    "```*es -> *   # remove trailing es```  \n",
    "```*ies -> *   # remove trailing ies```  \n",
    "```*ing -> *   # remove trailing ing```  \n",
    "```*al -> *   # remove trailing al```  \n",
    "```*ity -> *   # remove trailing ity```  \n",
    "```*ly -> *   # remove trailing ly```  \n",
    "```*y -> *i   # replace trailing y by i```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Use the cleaned word tokens (Step 5 above) and apply stemming. Use the Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Compare the results of the Snowball Stemmer with your stemming rules. How do they differ, how could you improve your stemmer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "Create the word clouds from Exercise 1 again, but now with the preprocessed text.\n",
    "\n",
    "What changes do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_word_cloud\n",
    "\n",
    "# CODE SUBMISSION ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Zipf's Law (5 P):\n",
    "\n",
    "In the lecture, you have heard about Zipf’s law. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "State Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit) - expected approx. 1-2 sentences and the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Check if Zipf's law (approximately) holds for our three datasets after all preprocessing steps.\n",
    "\n",
    "For this, plot Zipf's law and the word distribution for each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SUBMISSION ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "Describe your plots and discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TEXT SUBMISSION ANSWER HERE (Double click to edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting your results:\n",
    "\n",
    "To submit your results, please:\n",
    "\n",
    "- save this file, i.e., `ex??_assignment.ipynb`.\n",
    "- if you reference any external files (e.g., images), please create a zip or rar archieve and put the notebook files and all referenced files in there.\n",
    "- login to ILIAS and submit the `*.ipynb` or archive for the corresponding assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "    \n",
    "- Do not copy any code from the Internet. In case you want to use publicly available code, please, add the reference to the respective code snippet.\n",
    "- Check your code compiles and executes, even after you have restarted the Kernel.\n",
    "- Submit your written solutions and the coding exercises within the provided spaces and not otherwise.\n",
    "- Write the names of your partner and your name in the top section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
